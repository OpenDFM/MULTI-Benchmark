<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="MULTI-Benchmark"/>
  <meta property="og:description" content="MULTI: Multimodal Understanding Leaderboard with Text and Images"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="MULTI-Benchmark">
  <meta name="twitter:description" content="MULTI: Multimodal Understanding Leaderboard with Text and Images">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MULTI-Benchmark</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">-->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MULTI: Multimodal Understanding Leaderboard with Text and Images</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
            <!-- Paper authors -->
                <span class="author-block">
                    <a href="https://github.com/JamesZhutheThird" target="_blank">Zichen Zhu</a>,&nbsp;</span>
                <span class="author-block">
                  <a href="https://github.com/void-b583x2-NULL" target="_blank">Yang Xu</a>,&nbsp;</span>
                <span class="author-block">
                  <a href="https://coai-sjtu.github.io" target="_blank">Lu Chen</a><sup>†</sup>,&nbsp;</span>
                <span class="author-block">
                  <a href="https://github.com/Ethereal-O" target="_blank">Jingkai Yang</a>,&nbsp;</span>
                <span class="author-block">
                  <a href="https://github.com/Entarochuan" target="_blank">Yichuan Ma</a>,&nbsp;</span>
                <span class="author-block">
                  Yimin Sun,&nbsp;</span>
                <span class="author-block">
                  Hailin Wen,&nbsp;</span><br>
                <span class="author-block">
                  Jiaqi Liu,&nbsp;</span>
                <span class="author-block">
                  Jinyu Cai,&nbsp;</span>
                <span class="author-block">
                  <a href="https://gray311.github.io" target="_blank">Yingzi Ma</a>,&nbsp;</span>
                <span class="author-block">
                  Liangtai Sun,&nbsp;</span>
                <span class="author-block">
                  Zihan Zhao,&nbsp;</span>
                <span class="author-block">
                  <a href="https://cs.sjtu.edu.cn/PeopleDetail.aspx?id=76" target="_blank">Kai Yu</a><sup>†</sup>&nbsp;</span>
                </div>

                <div class="is-size-5 publication-authors">
                  <span class="author-block"><a href="https://X-LANCE.sjtu.edu.cn" target="_blank">X-LANCE Lab</a>, Department of Computer Science and Engineering&nbsp;&nbsp;<br> MoE Key Lab of Artificial Intelligence, SJTU AI Institute <br> Shanghai Jiao Tong University, Shanghai, China </span></br>
                  <span class="author-block">†Corresponding Authors</span><br>
                  <span class="author-block"><a href="mailto:JamesZhutheThird@sjtu.edu.cn">JamesZhutheThird@sjtu.edu.cn</a>,</span>
                  <span class="author-block"><a href="mailto:xuyang0112@sjtu.edu.cn">xuyang0112@sjtu.edu.cn</a>,</span></br>
                  <span class="author-block"><a href="mailto:chenlusz@sjtu.edu.cn">chenlusz@sjtu.edu.cn</a>,</span>
                  <span class="author-block"><a href="mailto:kai.yu@sjtu.edu.cn">kai.yu@sjtu.edu.cn</a></span>
              
                </div>


                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://x-lance.github.io/MULTI-Benchmark/" target="_blank" class="external-link button is-normal is-rounded">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming Soon)</span>
                </a>
              </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/X-LANCE/MULTI-Benchmark" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>


                <!-- HuggingFace Link -->
                <span class="link-block">
                  <a href="https://x-lance.github.io/MULTI-Benchmark/" target="_blank" class="external-link button is-normal is-rounded">
                  <span class="icon">
                    <p style="font-size:20px">&#x1F917;</p>
                  </span>
                  <span>Dataset (Coming Soon)</span>
                </a>
              </span>

              <!-- HuggingFace Link -->
                <span class="link-block">
                  <a href="https://x-lance.github.io/MULTI-Benchmark/" target="_blank" class="external-link button is-normal is-rounded">
                  <span class="icon">
                    <p style="font-size:20px">&#x1F917;</p>
                  </span>
                  <span>Leaderboard (Coming Soon)</span>
                </a>
              </span>


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="publication-flipped">-->
<!--        <div class="content has-text-justified">-->
<!--&lt;!&ndash;            <p style="text-align:center;">&ndash;&gt;-->
<!--&lt;!&ndash;               <img src="static/images/1_fig1.png"  style="width: 80%; height: 80%"/>&ndash;&gt;-->
<!--&lt;!&ndash;               <br>&ndash;&gt;-->
<!--&lt;!&ndash;            </p>&ndash;&gt;-->

<!--        </div>-->

<!--      </div>-->
<!--    </div>-->
    <h3 class="subtitle is-size-3-tablet has-text-left pb-">
    <p style="text-align:justify; line-height:150%; margin-left: -130px; margin-right: -130px; font-size: 20px">We introduce <b>MULTI</b>: a multi-level, multi-disciplinary, and multi-type cross-modal test benchmark, aimed at evaluating the performance of multimodal generative large models under different conditions and scenarios. We collected and organized nearly 18K questions from exams, quizzes, textbooks and other educational resources, most of which underwent at least two rounds of human annotation and proofreading, and three rounds of script cleaning. Some questions were manually adapted to make them more suitable for evaluating the comprehensive ability of the model. These questions involve four educational levels: junior high school, high school, college and social exams, covering Chinese, mathematics, English, physics, chemistry, biology, history, geography, politics, information technology, driving test and other disciplines and fields, including single choice, multiple choices, fill in the blank (given range and fully open), and open-ended comprehensive questions.
      <br><br>We manually selected 500 questions to form a difficult subset, which is used to evaluate the model's extreme performance. These questions often contain multiple images and formulas, test the model's comprehensive understanding of multiple images, and require complex and rigorous logical reasoning. The performance of this part of the data will be displayed separately on the leaderboard.
      <br><br>We tested on GPT-3.5 and open-source multimodal large models<sup>*</sup>, and the results show that even the advanced GPT-3.5 only achieved 43.28% accuracy, showing a huge room for improvement. We believe that MULTI will motivate the community to build the next generation of multimodal foundation models, to achieve expert-level artificial general intelligence.
      <br><br> <p style="font-size:15px"><sup>*</sup> Based on v0.3.0-20231115 version of the data, tested on SC/MC/FIB three question types.</p>
  </div><h3>
</section>



<!-- Paper abstract -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="publication-flipped">
        <div class="content has-text-justified">
            <h3 class="subtitle is-size-3-tablet has-text-left pb-5">
               <p>
                   <br>
                    How can I early access MULTI 🤔?<br>
               </p>
            </h3>
            <h3 class="subtitle is-size-4-tablet has-text-left pb-5">
               <p>
                   Please feel free to contact (<a href="mailto:JamesZhutheThird@sjtu.edu.cn">JamesZhutheThird@sjtu.edu.cn</a>) and keep in touch with us.
               </p>
            </h3>
        </div>
      </div>
    </div>
  </div>
</section>


<!--&lt;!&ndash; Image carousel &ndash;&gt;-->
<!--<section class="hero is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <div id="results-carousel" class="carousel results-carousel">-->

<!--        <div class="item">-->
<!--         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">-->
<!--           VisIT-Bench Instance-->
<!--           </h2>-->
<!--         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--         <p>-->
<!--             An example from VisIT-Bench, featuring an image, an instruction, an  “instruction-conditioned caption”, a detailed description allowing a model to follow the instruction using just the text, and a human-verified response from GPT-4. These elements are used for evaluating multimodal chatbots and updating a leaderboard.📊-->

<!--         </p>-->
<!--         <p style="text-align:center;">-->
<!--           <br><br>-->
<!--           <img src="static/images/2_example.png"  style="width: 100%; height: 100%"/>-->
<!--         </p>-->
<!--         </h3>-->
<!--      </div>-->

<!--      <div class="item">-->
<!--         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">-->
<!--           Data Collection Framework-->
<!--           </h2>-->
<!--         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--         <p>-->
<!--             1. Creating “wish-list” instructions for desired V&L chatbot capabilities<br>-->
<!--             2. Using these as inspiration for instructions annotation<br>-->
<!--             3. Collecting instruction-conditioned dense captions<br>-->
<!--             4. Generating human-verified chatbot responses from GPT-4 outputs<br>-->
<!--         </p>-->
<!--         <p style="text-align:center;">-->
<!--           <br><br>-->
<!--           <img src="static/images/3_dataset_collection.png"  style="width: 80%; height: 80%"/>-->
<!--         </p>-->
<!--         </h3>-->
<!--      </div>-->

<!--      <div class="item">-->
<!--         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">-->
<!--           Repurposing Existing Datasets-->
<!--           </h2>-->
<!--         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--         <p>-->
<!--            VisIT-Bench repurposes 25 datasets to a chatbot-style, including 10 multi-image datasets.<br>-->
<!--             Here, we add an instruction prompt and a chatbot response to an NLVR2 instance.<br>-->
<!--             This methodology leverages previous studies, tailoring them to current chatbot requirements.-->
<!--         </p>-->
<!--         <p style="text-align:center;">-->
<!--           <br><br>-->
<!--           <img src="static/images/4_nlvr_image.png"  style="width: 100%; height: 100%"/>-->
<!--         </p>-->
<!--         </h3>-->
<!--      </div>-->

<!--      <div class="item">-->
<!--         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">-->
<!--             Instruction-Conditioned Dense Captions and Data Collection Results-->
<!--           </h2>-->
<!--         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--         <p>-->
<!--             With a 91.5% success rate in single-image scenarios, our data collection demonstrates the effectiveness of instruction-conditioned dense captions.<br>-->
<!--             It also showcases the necessity of our dense captions over generated captions from a SoTA BLIP2 captioning model. 📈-->
<!--         </p>-->
<!--         <p style="text-align:center;">-->
<!--           <br><br>-->
<!--             <img src="static/images/5_dataset_results_plus_ablation.png"  style="width: 100%; height: 100%"/>-->

<!--             &lt;!&ndash; <br> &ndash;&gt;-->
<!--             &lt;!&ndash; <img src="static/images/5_necessity_of_captions.png"  style="width: 90%; height: 90%"/> &ndash;&gt;-->

<!--         </p>-->
<!--         </h3>-->
<!--      </div>-->

<!--      <div class="item">-->
<!--         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">-->
<!--           Features of the Dataset-->
<!--           </h2>-->
<!--         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--           <p>-->
<!--               VisIT-Bench emphasizes diverse tasks and human-chatbot interactions.<br>-->
<!--               We stand apart with our 'wish-list' instructions, 70 tested skills, and the repurposing of existing datasets, including multi-image tasks, thereby reflecting the dynamic demands of modern chatbots.🎯.-->
<!--           </p>-->
<!--         <p style="text-align:center;">-->
<!--           <br><br>-->
<!--           <img src="static/images/7_table_compare.png"  style="width: 100%; height: 100%"/>-->
<!--         </p>-->
<!--         </h3>-->
<!--      </div>-->

<!--      <div class="item">-->
<!--        <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">-->
<!--           Human-Guided Rankings-->
<!--           </h2>-->
<!--         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--           <p>-->
<!--               VisIT-Bench facilitates the comparison of different V&L models.<br>-->
<!--               By using human preference annotations, we form a leaderboard, providing insights into the strengths and weaknesses of each model in various tasks.-->
<!--           </p>-->
<!--         <p style="text-align:center;">-->
<!--           <br><br>-->
<!--           <img src="static/images/8_human_preferences.png"  style="width: 90%; height: 90%"/>-->
<!--         </p>-->
<!--         </h3>-->
<!--      </div>-->

<!--      <div class="item">-->
<!--        <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">-->
<!--            Automatic Evaluation and Dynamic Leaderboard-->
<!--           </h2>-->
<!--         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--           <p>-->
<!--               Using GPT-4 as a judge, we host head-to-head battles among top vision-and-language models 🥊.<br>-->
<!--               Our leaderboard reflects human preferences with high agreement, making it a scalable and reliable assessment tool.⚖️-->
<!--           </p>-->
<!--         <p style="text-align:center;">-->
<!--           <br><br>-->
<!--           <img src="static/images/9_auto_eval_expanded.png"  style="width: 85%; height: 85%"/>-->
<!--             &lt;!&ndash; <br> &ndash;&gt;-->
<!--           &lt;!&ndash; <img src="static/images/10_auto_eval_expanded.png"  style="width: 60%; height: 60%"/> &ndash;&gt;-->
<!--         </p>-->
<!--         </h3>-->
<!--      </div>-->

<!--       <div class="item">-->
<!--        <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">-->
<!--           Evaluating Evaluation Metrics-->
<!--           </h2>-->
<!--         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--           <p>-->
<!--               How good is our automatic metric? We measure correlations of several automatic metrics vs. human preferences, with our reference free evaluation (GPT-4-no-ref) showing the strongest alignment (top orange line - upper bound, bottom blue line - random chance (50%).📏-->
<!--           </p>-->
<!--         <p style="text-align:center;">-->
<!--           <br><br>-->
<!--           <img src="static/images/10_correlation.png"  style="width: 80%; height: 80%"/>-->
<!--         </p>-->
<!--         </h3>-->
<!--      </div>-->

<!--      <div class="item">-->
<!--        <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">-->
<!--           Results by Different Models on Different Instruction Families-->
<!--           </h2>-->
<!--         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--           <p>-->
<!--               VisIT-Bench offers detailed insight into the performance of V&L models.<br>-->
<!--               Through our diverse instruction families, you can assess how different models perform on various tasks, providing a thorough understanding of their capabilities.🔍-->
<!--           </p>-->
<!--         <p style="text-align:center;">-->
<!--           <br><br>-->
<!--           <img src="static/images/11_res_per_category.png"  style="width: 100%; height: 100%"/>-->
<!--         </p>-->
<!--         </h3>-->
<!--      </div>-->

<!--  </div>-->
<!--</div>-->
<!--</div>-->
<!--</section>-->
<!--&lt;!&ndash; End image carousel &ndash;&gt;-->

<!--&lt;!&ndash; Start explorer embedding &ndash;&gt;-->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--       <h3 class="subtitle is-size-1-tablet has-text-weight-bold has-text-centered pr-4 pl-4 pt-3 pb-3">-->
<!--            Dataset Viewer-->
<!--        </h3>-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--          <div class="publication-video">-->
<!--            &lt;!&ndash; <gradio-app src="https://nlphuji-whoops-dataset-viewer.hf.space"></gradio-app> &ndash;&gt;-->
<!--            <gradio-app src="https://mlfoundations-visit-bench-dataset-viewer.hf.space"></gradio-app>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!--&lt;!&ndash; End explorer embedding &ndash;&gt;-->


<!--&lt;!&ndash; Start explorer embedding &ndash;&gt;-->
<!--<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.35.2/gradio.js"></script>-->
<!--<section class="hero is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--        <h3 class="subtitle is-size-1-tablet has-text-weight-bold has-text-centered pr-4 pl-4 pt-3 pb-3">-->
<!--            Leaderboard-->
<!--        </h3>-->
<!--        <h3 class="subtitle is-size-5-tablet has-text-weight-bold">-->
<!--            To submit your results to the leaderboard, please add a "predictions" column to <a href="https://huggingface.co/datasets/mlfoundations/VisIT-Bench/blob/main/visit_bench_single_image.csv">this csv</a>, and send to <a href="mailto:yonatanbitton1@gmail.com">this mail</a>.-->
<!--        </h3>-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--          <div class="publication-video">-->
<!--            &lt;!&ndash; <gradio-app src="https://mkshing-jp-llm-leaderboard.hf.space"></gradio-app> &ndash;&gt;-->
<!--            <gradio-app src="https://mlfoundations-visit-bench-leaderboard.hf.space"></gradio-app>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!--&lt;!&ndash; End explorer embedding &ndash;&gt;-->

<!--<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.16.2/gradio.js"></script>-->
<!-- Youtube video -->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--          <div class="publication-video">-->
<!--          <h3 class="subtitle is-size-4-tablet has-text-left">-->
<!--               <p>-->
<!--                   q2d's auto-generated dialogs enable query generation models to adapt and improve for specific dialog styles, creating labeled datasets for training and evaluation.-->
<!--                   <br>T5 model predictions above/below the line show the impact of fine-tuning on MuSiQue dialogs.-->
<!--               </p>-->
<!--               <p style="text-align:center;">-->
<!--                   <br><br>-->
<!--                   <img src="static/images/q2d_5.png"  style="width: 60%; height: 60%"/>-->
<!--               </p>-->
<!--         </h3>-->
<!--          </div>-->
<!--        </div>-->

<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!-- End youtube video-->

<!-- Youtube video -->
<!--<section class="hero is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h3 class="subtitle is-size-4-tablet has-background-info-light has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--      We collect <i>normal</i> (synthetic, not weird) and <i>natural</i> (non-synthetic, not weird) images to investigate the main challenge in WHOOPS!. BLIP2 model performs well on <i>non-weird</i> cases but struggles on weird ones, indicating that weirdness is the primary challenge, not synthesis.-->
<!--      </h3>-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--          <div class="publication-video">-->
<!--            <iframe src="https://nlphuji-wmtis-explorer-identify.hf.space" frameborder="0" width="850" height="450"></iframe>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->


<!-- Paper poster -->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title">Paper</h2>-->
<!--      <iframe  src="static/pdfs/WHOOPS_paper.pdf" width="100%" height="550">-->
<!--          </iframe>-->
<!--        -->
<!--      </div>-->
<!--    </div>-->
<!--  </section>-->
<!--End paper poster -->

<!--BibTex citation -->
<section class="section" id="BibTeX">
<div class="container is-max-desktop content">
<h2 class="title">BibTeX</h2>
<pre><code>@misc{zhu2023multibench,
      title={MULTI: Multimodal Understanding Leaderboard with Text and Images},
      author={Zichen Zhu, Yang Xu, Lu Chen, Jingkai Yang, Yichuan Ma, Yimin Sun, Hailin Wen, Jiaqi Liu, Jinyu Cai, Yingzi Ma, Liangtai Sun, Zihan Zhao, Kai Yu},
      year={2023},
        howpublished = "\url{https://github.com/X-LANCE/MULTI-Benchmark}",
<!--      eprint={2308.06595},-->
<!--      archivePrefix={arXiv},-->
<!--      primaryClass={cs.CL}-->
}</code>
</div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p style="color:gray;font-size:9.9px;">
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
  </html>
