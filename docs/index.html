<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="MULTI-Benchmark"/>
    <meta property="og:description" content="MULTI: Multimodal Understanding Leaderboard with Text and Images"/>
    <meta property="og:url" content="URL OF THE WEBSITE"/>

    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/overview.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>


    <meta name="twitter:title" content="MULTI-Benchmark">
    <meta name="twitter:description" content="MULTI: Multimodal Understanding Leaderboard with Text and Images">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/overview.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>MULTI-Benchmark</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/sort-table.js" defer></script>
    <script src="static/js/index.js"></script>
</head>

<body>

<!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">-->

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title"><b>MULTI</b>: Multimodal Understanding
                        Leaderboard with Text and Images</h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <!-- Paper authors -->
                        <span class="author-block">
                                <a href="https://github.com/JamesZhutheThird" target="_blank">Zichen
                                    Zhu</a>,&nbsp;</span>
                        <span class="author-block">
                                <a href="https://github.com/void-b583x2-NULL" target="_blank">Yang Xu</a>,&nbsp;</span>
                        <span class="author-block">
                                <a href="https://coai-sjtu.github.io" target="_blank">Lu
                                    Chen</a><sup>â€ </sup>,&nbsp;</span>
                        <span class="author-block">
                                <a href="https://github.com/Ethereal-O" target="_blank">Jingkai Yang</a>,&nbsp;</span>
                        <span class="author-block">
                                <a href="https://github.com/Entarochuan" target="_blank">Yichuan Ma</a>,&nbsp;</span>
                        <span class="author-block">
                                Yiming Sun,&nbsp;</span>
                        <span class="author-block">
                                Hailin Wen,&nbsp;</span><br>
                        <span class="author-block">
                                Jiaqi Liu,&nbsp;</span>
                        <span class="author-block">
                                Jinyu Cai,&nbsp;</span>
                        <span class="author-block">
                                <a href="https://gray311.github.io" target="_blank">Yingzi Ma</a>,&nbsp;</span>
                        <span class="author-block">
                                Situo Zhang,&nbsp;</span>
                        <span class="author-block">
                                Zihan Zhao,&nbsp;</span>
                        <span class="author-block">
                                Liangtai Sun,&nbsp;</span>
                        <span class="author-block">
                                <a href="https://cs.sjtu.edu.cn/PeopleDetail.aspx?id=76" target="_blank">Kai
                                    Yu</a><sup>â€ </sup>&nbsp;</span>
                    </div>

                    <div class="is-size-5 publication-authors">
                            <span class="author-block"><a href="https://X-LANCE.sjtu.edu.cn" target="_blank">X-LANCE
                                    Lab</a>, Department of Computer Science and Engineering&nbsp;&nbsp;<br> MoE Key Lab
                                of Artificial Intelligence, SJTU AI Institute <br> Shanghai Jiao Tong University,
                                Shanghai, China </span></br>
                        <span class="author-block">â€ Corresponding Authors</span><br>
                        <span class="author-block"><a
                                href="mailto:JamesZhutheThird@sjtu.edu.cn">JamesZhutheThird@sjtu.edu.cn</a>,</span>
                        <span class="author-block"><a
                                href="mailto:xuyang0112@sjtu.edu.cn">xuyang0112@sjtu.edu.cn</a>,</span></br>
                        <span class="author-block"><a
                                href="mailto:chenlusz@sjtu.edu.cn">chenlusz@sjtu.edu.cn</a>,</span>
                        <span class="author-block"><a href="mailto:kai.yu@sjtu.edu.cn">kai.yu@sjtu.edu.cn</a></span>

                    </div>


                    <!-- ArXiv abstract Link -->
                    <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.03173/" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                    <!-- Github link -->
                    <span class="link-block">
                            <a href="https://github.com/OpenDFM/MULTI-Benchmark" target="_blank"
                               class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                    <i class="fab fa-github"></i>
                                </span>
                                <span>Code</span>
                            </a>
                        </span>


                    <!-- HuggingFace Link -->
                    <span class="link-block">
                            <a href="https://huggingface.co/datasets/OpenDFM/MULTI-Benchmark/" target="_blank"
                               class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                    <p style="font-size:20px">&#x1F917;</p>
                                </span>
                                <span>Dataset</span>
                            </a>
                        </span>

                    <!-- HuggingFace Link -->
                    <span class="link-block">
                            <a href="https://github.com/OpenDFM/MULTI-Benchmark/tree/main?tab=readme-ov-file#-leaderboard" target="_blank"
                               class="external-link button is-normal is-rounded">
                                <span class="icon">
                                    <p style="font-size:20px">&#x1F917;</p>
                                </span>
                                <span>Leaderboard (Coming Soon)</span>
                            </a>
                        </span>


                </div>
            </div>
        </div>
    </div>
    </div>
    </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-small is-light">
    <div class="container is-max-desktop">
        <h3 class="subtitle is-size-3-tablet has-text-left pb-">

            <p style="text-align:justify; line-height:150%; margin-left: -75px; margin-right: -75px; font-size: 20px">
                <img src="static/images/overview.png"/>

                <br><br>
                Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community, while existing benchmarks primarily focus on understanding simple natural images and short context. In this paper, we present <i><b>MULTI</b></i>, as a cutting-edge benchmark for evaluating MLLMs on understanding complex tables and images, and reasoning with long context. <b>MULTI</b> provides multimodal inputs and requires responses that are either precise or open-ended, reflecting real-life examination styles. <b>MULTI</b> includes over 18,000 questions and challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis and cross-modality reasoning. We also introduce <i><b>MULTI-Elite</b></i>, a 500-question selected hard subset, and <i><b>MULTI-Extend</b></i>, with more than 4,500 external knowledge context pieces. Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a <b>63.7%</b> accuracy rate on <b>MULTI</b>, in contrast to other MLLMs scoring between <b>28.5%</b> and <b>55.3%</b>. <b>MULTI</b> serves not only as a robust evaluation platform but also paves the way for the development of expert-level AI.
            </p>
    </div>
    <h3>
</section>


<!-- Image carousel -->
<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel">

                <div class="item">
                    <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
                        Statistics </h2>

                 <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
                        <p style="text-align:center;">
                            <img src="static/images/stat.png" style="width: 60%; height: 100%"/>
                        </p>
                        <p>
                            <br>
                            <b>MULTI</b> consist of more than 18K questions and 8K images, covering
                                    23 subjects and 4 educational levels. <b>MULTI</b> is one of the largest
                                    Chinese multimodal datasets in complex scientific reasoning and image understanding.
                        </p>
                    </h3>
                </div>



                <div class="item">
                    <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
                        Annotation Platform
                    </h2>
                    <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
                        <p style="text-align:center;">
                            <img src="static/images/platform.png" style="width: 80%; height: 100%"/>
                        </p>

                        <p>
                            <br>
                            Our annotation platform is designed to support editing and rendering complex MarkDown
                            formats, and it's easy to check and update question property in detail.
                        </p>
                    </h3>
                </div>



                <div class="item">
                    <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
                        Annotation Pipeline
                    </h2>
                    <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
                        <p style="text-align:center;">
                            <br><br><br><br>
                            <img src="static/images/pipeline.png" style="width: 100%; height: 100%"/>
                        </p>

<!--                        <p>-->
<!--                            Our annotation pipeline.-->
<!--                        </p>-->
                    </h3>
                </div>



                <div class="item">
                    <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
                        Annotation Examples
                    </h2>
                    <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
                        <p style="text-align:center;">
                            <br>
                            <img src="static/images/data_anno.png" style="width: 80%; height: 100%"/>
                        </p>

<!--                        <p>-->
<!--                            Our annotation pipeline.-->
<!--                        </p>-->
                    </h3>
                </div>





                <div class="item">
                    <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
                        Post-procession Examples
                    </h2>
                    <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
                        <p style="text-align:center;">
                            <br><br>
                            <img src="static/images/data_aug.png" style="width: 80%; height: 100%"/>
                        </p>

<!--                        <p>-->
<!--                            Our annotation pipeline.-->
<!--                        </p>-->
                    </h3>
                </div>




                <div class="item">
                    <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
                        Question Examples
                    </h2>
                    <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
                        <p style="text-align:center;">
                            <img src="static/images/examples.png" style="width: 80%; height: 100%"/>
                        </p>

<!--                        <p>-->
<!--                            Our annotation pipeline.-->
<!--                        </p>-->
                    </h3>
                </div>


                <div class="item">
                    <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
                        Prompts
                    </h2>
                    <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
                        <p style="text-align:center;">
                            <img src="static/images/prompts_all.png" style="width: 80%; height: 100%"/>
                        </p>

<!--                        <p>-->
<!--                            Our annotation pipeline.-->
<!--                        </p>-->
                    </h3>
                </div>

            </div>
        </div>
    </div>
</section>
<!-- End image carousel -->

<!--&lt;!&ndash; Start explorer embedding &ndash;&gt;-->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--       <h3 class="subtitle is-size-1-tablet has-text-weight-bold has-text-centered pr-4 pl-4 pt-3 pb-3">-->
<!--            Dataset Viewer-->
<!--        </h3>-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--          <div class="publication-video">-->
<!--            &lt;!&ndash; <gradio-app src="https://nlphuji-whoops-dataset-viewer.hf.space"></gradio-app> &ndash;&gt;-->
<!--            <gradio-app src="https://mlfoundations-visit-bench-dataset-viewer.hf.space"></gradio-app>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!--&lt;!&ndash; End explorer embedding &ndash;&gt;-->


<!--&lt;!&ndash; Start explorer embedding &ndash;&gt;-->
<!--<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.35.2/gradio.js"></script>-->
<!--<section class="hero is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--        <h3 class="subtitle is-size-1-tablet has-text-weight-bold has-text-centered pr-4 pl-4 pt-3 pb-3">-->
<!--            Leaderboard-->
<!--        </h3>-->
<!--        <h3 class="subtitle is-size-5-tablet has-text-weight-bold">-->
<!--            To submit your results to the leaderboard, please add a "predictions" column to <a href="https://huggingface.co/datasets/mlfoundations/VisIT-Bench/blob/main/visit_bench_single_image.csv">this csv</a>, and send to <a href="mailto:yonatanbitton1@gmail.com">this mail</a>.-->
<!--        </h3>-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--          <div class="publication-video">-->
<!--            &lt;!&ndash; <gradio-app src="https://mkshing-jp-llm-leaderboard.hf.space"></gradio-app> &ndash;&gt;-->
<!--            <gradio-app src="https://mlfoundations-visit-bench-leaderboard.hf.space"></gradio-app>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!--&lt;!&ndash; End explorer embedding &ndash;&gt;-->

<!--<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.16.2/gradio.js"></script>-->
<!-- Youtube video -->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--          <div class="publication-video">-->
<!--          <h3 class="subtitle is-size-4-tablet has-text-left">-->
<!--               <p>-->
<!--                   q2d's auto-generated dialogs enable query generation models to adapt and improve for specific dialog styles, creating labeled datasets for training and evaluation.-->
<!--                   <br>T5 model predictions above/below the line show the impact of fine-tuning on MuSiQue dialogs.-->
<!--               </p>-->
<!--               <p style="text-align:center;">-->
<!--                   <br><br>-->
<!--                   <img src="static/images/q2d_5.png"  style="width: 60%; height: 60%"/>-->
<!--               </p>-->
<!--         </h3>-->
<!--          </div>-->
<!--        </div>-->

<!--      </div>-->
<!--    </div>-->
<!--</section>-->
<!-- End youtube video-->

<!-- Youtube video -->
<!--<section class="hero is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h3 class="subtitle is-size-4-tablet has-background-info-light has-text-left pr-4 pl-4 pt-3 pb-3">-->
<!--      We collect <i>normal</i> (synthetic, not weird) and <i>natural</i> (non-synthetic, not weird) images to investigate the main challenge in WHOOPS!. BLIP2 model performs well on <i>non-weird</i> cases but struggles on weird ones, indicating that weirdness is the primary challenge, not synthesis.-->
<!--      </h3>-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--          <div class="publication-video">-->
<!--            <iframe src="https://nlphuji-wmtis-explorer-identify.hf.space" frameborder="0" width="850" height="450"></iframe>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--</section>-->


<!-- Paper poster -->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title">Paper</h2>-->
<!--      <iframe  src="static/pdfs/WHOOPS_paper.pdf" width="100%" height="550">-->
<!--          </iframe>-->
<!--        -->
<!--      </div>-->
<!--    </div>-->
<!--  </section>-->
<!--End paper poster -->



<!-- Paper abstract -->
<section class="section hero is-small">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="publication-flipped">
                <div class="content has-text-justified">
                    <h3 class="subtitle is-size-3-tablet has-text-left pb-5">
                        <p>
                            <br>
                            How can I access <b>MULTI</b> ðŸ¤”?<br>
                        </p>
                    </h3>
                    <h3 class="subtitle is-size-4-tablet has-text-left pb-5">
                        <p>
                            Please visit our <a href="https://huggingface.co/datasets/OpenDFM/MULTI-Benchmark/">HuggingFace</a> page to access <b>MULTI</b> dataset. Our code is available on <a href="https://github.com/OpenDFM/MULTI-Benchmark">GitHub</a>.<br>
                        </p>
                    </h3>
                </div>
            </div>
        </div>
    </div>
</section>





<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@misc{zhu2024multi,
      title={MULTI: Multimodal Understanding Leaderboard with Text and Images},
      author={Zichen Zhu and Yang Xu and Lu Chen and Jingkai Yang and Yichuan Ma and Yiming Sun and Hailin Wen and Jiaqi Liu and Jinyu Cai and Yingzi Ma and Situo Zhang and Zihan Zhao and Liangtai Sun and Kai Yu},
      year={2024},
      eprint={2402.03173},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}</code>
    </div>
</section>

<!-- <div class="columns is-centered m-6">
    <div class="column is-full has-text-centered content">
      <h2 class="title is-3" id="leaderboard">Leaderboard</h2>
      <div class="content">
        <table id="table1" class="js-sort-table">
            <tr>
            <td class="js-sort-number"><strong>Reset</strong></td>
            <td class="js-sort-number"><strong>Test Overall</strong></td>
            <td class="js-sort-number"><strong>Validation Overall</strong></td>
            <td class="js-sort-number"><strong>Art & Design</strong></td>
            <td class="js-sort-number"><strong>Business</strong></td>
            <td class="js-sort-number"><strong>Science</strong></td>
            <td class="js-sort-number"><strong>Health & Medicine</strong></td>
            <td class="js-sort-number"><strong>Human & Social Sci.</strong></td>
            <td class="js-sort-number"><strong>Tech & Eng.</strong></td>
            </tr>
            <tr style="background-color: #def9cb;">
                <td style="text-align: left;">
                    <b> GPT-4V </b>
                </td>
                <td> <b>43.7</b> </td>
                <td> <b>42.5</b> </td>

                <td> 61.0 </td>
                <td> <b>36.3</b> </td>
                <td> <b>40.9</b> </td>
                <td> <b>46.8</b> </td>
                <td> <b>44.2</b> </td>
                <td> <b>41.5</b> </td>
            </tr>
            <tr style="background-color: #def9cb;">
                <td style="text-align: left;">
                        <b>Qwen-VL-Plus </b>
                </td>
                <td> 36.8 </td>
                <td> 39.5 </td>

                <td> 61.5 </td>
                <td> 23.2 </td>
                <td> 32.8 </td>
                <td> 40.5 </td>
                <td> 43.4 </td>
                <td> 33.3 </td>
            </tr>
        </table>
    </div>
  </div>
</div> -->

<!--End BibTex citation -->

<!--      eprint={2308.06595},-->
<!--      archivePrefix={arXiv},-->
<!--      primaryClass={cs.CL}-->
<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">

                    <p style="color:gray;font-size:9.9px;">
                        This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->
</body>
</html>